"""
Text Abstractor - Comprehensive NLP-based Summarization Tool
=============================================================

This tool provides both extractive and abstractive text summarization
using multiple state-of-the-art NLP techniques.

Features:
- Extractive Summarization: TF-IDF, TextRank, LexRank, LSA
- Abstractive Summarization: T5, BART, PEGASUS
- Preprocessing: Sentence tokenization, cleaning, normalization
- Multi-document summarization support
- Customizable summary length and compression ratio


import re
import math
import numpy as np
import warnings
from typing import List, Dict, Tuple, Optional, Union
from collections import Counter, defaultdict
from dataclasses import dataclass
import heapq

warnings.filterwarnings('ignore')

# Try to import advanced libraries (optional dependencies)
try:
    import nltk
    from nltk.tokenize import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    NLTK_AVAILABLE = True
    
    # Download required NLTK data
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet', quiet=True)
        
except ImportError:
    NLTK_AVAILABLE = False
    print("Warning: NLTK not available. Using basic tokenization.")

try:
    from transformers import (
        T5Tokenizer, T5ForConditionalGeneration,
        BartTokenizer, BartForConditionalGeneration,
        PegasusTokenizer, PegasusForConditionalGeneration,
        AutoTokenizer, AutoModel
    )
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: Transformers not available. Abstractive methods will be limited.")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.decomposition import TruncatedSVD
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: scikit-learn not available. Some extractive methods will be limited.")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    print("Warning: NetworkX not available. TextRank will use custom implementation.")


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class SummaryResult:
    """Container for summary results"""
    summary: str
    method: str
    original_length: int
    summary_length: int
    compression_ratio: float
    sentences_used: List[str]
    scores: Optional[Dict[str, float]] = None


# ============================================================================
# TEXT PREPROCESSING
# ============================================================================

class TextPreprocessor:
    """Handles text cleaning and preprocessing"""
    
    def __init__(self, use_stemming: bool = False, use_lemmatization: bool = True):
        self.use_stemming = use_stemming
        self.use_lemmatization = use_lemmatization
        
        if NLTK_AVAILABLE:
            self.stemmer = PorterStemmer() if use_stemming else None
            self.lemmatizer = WordNetLemmatizer() if use_lemmatization else None
            try:
                self.stop_words = set(stopwords.words('english'))
            except:
                self.stop_words = self._get_basic_stopwords()
        else:
            self.stemmer = None
            self.lemmatizer = None
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> set:
        """Basic stopwords list if NLTK is not available"""
        return set([
            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 
            'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',
            'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',
            'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',
            'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',
            'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',
            'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',
            'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',
            'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',
            'further', 'then', 'once'
        ])
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep sentence structure
        text = re.sub(r'[^\w\s.!?,-]', '', text)
        return text.strip()
    
    def tokenize_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        if NLTK_AVAILABLE:
            return sent_tokenize(text)
        else:
            # Basic sentence tokenization
            sentences = re.split(r'[.!?]+', text)
            return [s.strip() for s in sentences if s.strip()]
    
    def tokenize_words(self, text: str) -> List[str]:
        """Split text into words"""
        if NLTK_AVAILABLE:
            return word_tokenize(text.lower())
        else:
            return re.findall(r'\b\w+\b', text.lower())
    
    def remove_stopwords(self, words: List[str]) -> List[str]:
        """Remove stopwords from word list"""
        return [w for w in words if w not in self.stop_words]
    
    def normalize_word(self, word: str) -> str:
        """Normalize a word using stemming or lemmatization"""
        if self.use_stemming and self.stemmer:
            return self.stemmer.stem(word)
        elif self.use_lemmatization and self.lemmatizer:
            return self.lemmatizer.lemmatize(word)
        return word
    
    def preprocess_text(self, text: str, remove_stops: bool = True) -> List[str]:
        """Complete preprocessing pipeline"""
        words = self.tokenize_words(text)
        if remove_stops:
            words = self.remove_stopwords(words)
        words = [self.normalize_word(w) for w in words]
        return words


# ============================================================================
# EXTRACTIVE SUMMARIZATION - TF-IDF METHOD
# ============================================================================

class TFIDFSummarizer:
    """Extractive summarization using TF-IDF scoring"""
    
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
    
    def calculate_tf(self, words: List[str]) -> Dict[str, float]:
        """Calculate term frequency"""
        word_count = Counter(words)
        total_words = len(words)
        return {word: count / total_words for word, count in word_count.items()}
    
    def calculate_idf(self, sentences: List[str]) -> Dict[str, float]:
        """Calculate inverse document frequency"""
        word_doc_count = defaultdict(int)
        total_docs = len(sentences)
        
        for sentence in sentences:
            words = set(self.preprocessor.preprocess_text(sentence))
            for word in words:
                word_doc_count[word] += 1
        
        return {
            word: math.log(total_docs / (count + 1))
            for word, count in word_doc_count.items()
        }
    
    def calculate_sentence_scores(self, sentences: List[str]) -> Dict[int, float]:
        """Calculate TF-IDF scores for sentences"""
        idf = self.calculate_idf(sentences)
        sentence_scores = {}
        
        for idx, sentence in enumerate(sentences):
            words = self.preprocessor.preprocess_text(sentence)
            if not words:
                sentence_scores[idx] = 0
                continue
            
            tf = self.calculate_tf(words)
            score = sum(tf.get(word, 0) * idf.get(word, 0) for word in words)
            sentence_scores[idx] = score / len(words)  # Normalize by sentence length
        
        return sentence_scores
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """Generate extractive summary using TF-IDF"""
        original_text = text
        text = self.preprocessor.clean_text(text)
        sentences = self.preprocessor.tokenize_sentences(text)
        
        if len(sentences) <= num_sentences:
            return SummaryResult(
                summary=text,
                method="TF-IDF",
                original_length=len(original_text),
                summary_length=len(text),
                compression_ratio=1.0,
                sentences_used=sentences
            )
        
        scores = self.calculate_sentence_scores(sentences)
        
        # Select top sentences
        top_indices = heapq.nlargest(
            num_sentences, 
            scores.keys(), 
            key=lambda x: scores[x]
        )
        top_indices.sort()  # Maintain original order
        
        summary_sentences = [sentences[i] for i in top_indices]
        summary = ' '.join(summary_sentences)
        
        return SummaryResult(
            summary=summary,
            method="TF-IDF",
            original_length=len(original_text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(original_text),
            sentences_used=summary_sentences,
            scores={f"sentence_{i}": scores[i] for i in top_indices}
        )


# ============================================================================
# EXTRACTIVE SUMMARIZATION - TEXTRANK METHOD
# ============================================================================

class TextRankSummarizer:
    """Extractive summarization using TextRank algorithm"""
    
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.damping = 0.85
        self.min_diff = 1e-5
        self.steps = 100
    
    def sentence_similarity(self, sent1: str, sent2: str) -> float:
        """Calculate similarity between two sentences"""
        words1 = set(self.preprocessor.preprocess_text(sent1))
        words2 = set(self.preprocessor.preprocess_text(sent2))
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccard similarity
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def build_similarity_matrix(self, sentences: List[str]) -> np.ndarray:
        """Build similarity matrix for sentences"""
        n = len(sentences)
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    similarity_matrix[i][j] = self.sentence_similarity(
                        sentences[i], sentences[j]
                    )
        
        return similarity_matrix
    
    def textrank_scores(self, similarity_matrix: np.ndarray) -> np.ndarray:
        """Calculate TextRank scores using power iteration"""
        n = similarity_matrix.shape[0]
        
        # Normalize similarity matrix
        row_sums = similarity_matrix.sum(axis=1)
        norm_matrix = similarity_matrix / (row_sums[:, np.newaxis] + 1e-10)
        
        # Initialize scores
        scores = np.ones(n) / n
        
        # Power iteration
        for _ in range(self.steps):
            prev_scores = scores.copy()
            scores = (1 - self.damping) / n + self.damping * norm_matrix.T.dot(scores)
            
            if np.abs(scores - prev_scores).sum() < self.min_diff:
                break
        
        return scores
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """Generate extractive summary using TextRank"""
        original_text = text
        text = self.preprocessor.clean_text(text)
        sentences = self.preprocessor.tokenize_sentences(text)
        
        if len(sentences) <= num_sentences:
            return SummaryResult(
                summary=text,
                method="TextRank",
                original_length=len(original_text),
                summary_length=len(text),
                compression_ratio=1.0,
                sentences_used=sentences
            )
        
        # Build similarity matrix and calculate scores
        similarity_matrix = self.build_similarity_matrix(sentences)
        scores = self.textrank_scores(similarity_matrix)
        
        # Select top sentences
        top_indices = np.argsort(scores)[-num_sentences:]
        top_indices.sort()  # Maintain original order
        
        summary_sentences = [sentences[i] for i in top_indices]
        summary = ' '.join(summary_sentences)
        
        return SummaryResult(
            summary=summary,
            method="TextRank",
            original_length=len(original_text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(original_text),
            sentences_used=summary_sentences,
            scores={f"sentence_{i}": float(scores[i]) for i in top_indices}
        )


# ============================================================================
# EXTRACTIVE SUMMARIZATION - LSA (Latent Semantic Analysis)
# ============================================================================

class LSASummarizer:
    """Extractive summarization using Latent Semantic Analysis"""
    
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """Generate extractive summary using LSA"""
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learn is required for LSA summarization")
        
        original_text = text
        text = self.preprocessor.clean_text(text)
        sentences = self.preprocessor.tokenize_sentences(text)
        
        if len(sentences) <= num_sentences:
            return SummaryResult(
                summary=text,
                method="LSA",
                original_length=len(original_text),
                summary_length=len(text),
                compression_ratio=1.0,
                sentences_used=sentences
            )
        
        # Create TF-IDF matrix
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        
        # Apply SVD
        n_components = min(num_sentences, len(sentences))
        svd = TruncatedSVD(n_components=n_components, random_state=42)
        svd_matrix = svd.fit_transform(tfidf_matrix)
        
        # Calculate sentence scores based on singular values
        scores = np.zeros(len(sentences))
        for i in range(n_components):
            scores += np.abs(svd_matrix[:, i]) * svd.singular_values_[i]
        
        # Select top sentences
        top_indices = np.argsort(scores)[-num_sentences:]
        top_indices.sort()  # Maintain original order
        
        summary_sentences = [sentences[i] for i in top_indices]
        summary = ' '.join(summary_sentences)
        
        return SummaryResult(
            summary=summary,
            method="LSA",
            original_length=len(original_text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(original_text),
            sentences_used=summary_sentences,
            scores={f"sentence_{i}": float(scores[i]) for i in top_indices}
        )


# ============================================================================
# EXTRACTIVE SUMMARIZATION - LEXRANK METHOD
# ============================================================================

class LexRankSummarizer:
    """Extractive summarization using LexRank algorithm"""
    
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.threshold = 0.1
    
    def idf_modified_cosine(self, sentences: List[str]) -> np.ndarray:
        """Calculate IDF-modified cosine similarity matrix"""
        if not SKLEARN_AVAILABLE:
            # Fallback to simple Jaccard similarity
            n = len(sentences)
            similarity_matrix = np.zeros((n, n))
            for i in range(n):
                for j in range(n):
                    if i != j:
                        words1 = set(self.preprocessor.preprocess_text(sentences[i]))
                        words2 = set(self.preprocessor.preprocess_text(sentences[j]))
                        if words1 and words2:
                            intersection = words1.intersection(words2)
                            union = words1.union(words2)
                            similarity_matrix[i][j] = len(intersection) / len(union)
            return similarity_matrix
        
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        similarity_matrix = cosine_similarity(tfidf_matrix)
        return similarity_matrix
    
    def power_method(self, matrix: np.ndarray, threshold: float) -> np.ndarray:
        """Apply power method to calculate stationary distribution"""
        n = matrix.shape[0]
        
        # Threshold the matrix
        matrix = (matrix > threshold).astype(float)
        
        # Normalize rows
        row_sums = matrix.sum(axis=1)
        matrix = matrix / (row_sums[:, np.newaxis] + 1e-10)
        
        # Initialize uniform distribution
        scores = np.ones(n) / n
        
        # Power iteration
        for _ in range(100):
            prev_scores = scores.copy()
            scores = matrix.T.dot(scores)
            scores = scores / scores.sum()
            
            if np.abs(scores - prev_scores).sum() < 1e-6:
                break
        
        return scores
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """Generate extractive summary using LexRank"""
        original_text = text
        text = self.preprocessor.clean_text(text)
        sentences = self.preprocessor.tokenize_sentences(text)
        
        if len(sentences) <= num_sentences:
            return SummaryResult(
                summary=text,
                method="LexRank",
                original_length=len(original_text),
                summary_length=len(text),
                compression_ratio=1.0,
                sentences_used=sentences
            )
        
        # Calculate similarity matrix
        similarity_matrix = self.idf_modified_cosine(sentences)
        
        # Apply power method
        scores = self.power_method(similarity_matrix, self.threshold)
        
        # Select top sentences
        top_indices = np.argsort(scores)[-num_sentences:]
        top_indices.sort()  # Maintain original order
        
        summary_sentences = [sentences[i] for i in top_indices]
        summary = ' '.join(summary_sentences)
        
        return SummaryResult(
            summary=summary,
            method="LexRank",
            original_length=len(original_text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(original_text),
            sentences_used=summary_sentences,
            scores={f"sentence_{i}": float(scores[i]) for i in top_indices}
        )


# ============================================================================
# ABSTRACTIVE SUMMARIZATION - T5 MODEL
# ============================================================================

class T5Summarizer:
    """Abstractive summarization using T5 model"""
    
    def __init__(self, model_name: str = "t5-small"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library is required for T5 summarization")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)
    
    def summarize(
        self, 
        text: str, 
        max_length: int = 150, 
        min_length: int = 40,
        do_sample: bool = False
    ) -> SummaryResult:
        """Generate abstractive summary using T5"""
        # Prepare input
        input_text = "summarize: " + text
        inputs = self.tokenizer.encode(
            input_text, 
            return_tensors="pt", 
            max_length=512, 
            truncation=True
        ).to(self.device)
        
        # Generate summary
        summary_ids = self.model.generate(
            inputs,
            max_length=max_length,
            min_length=min_length,
            length_penalty=2.0,
            num_beams=4,
            do_sample=do_sample,
            early_stopping=True
        )
        
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        return SummaryResult(
            summary=summary,
            method="T5",
            original_length=len(text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(text),
            sentences_used=[summary]
        )


# ============================================================================
# ABSTRACTIVE SUMMARIZATION - BART MODEL
# ============================================================================

class BARTSummarizer:
    """Abstractive summarization using BART model"""
    
    def __init__(self, model_name: str = "facebook/bart-large-cnn"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library is required for BART summarization")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = BartTokenizer.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)
    
    def summarize(
        self, 
        text: str, 
        max_length: int = 150, 
        min_length: int = 40
    ) -> SummaryResult:
        """Generate abstractive summary using BART"""
        inputs = self.tokenizer.encode(
            text, 
            return_tensors="pt", 
            max_length=1024, 
            truncation=True
        ).to(self.device)
        
        summary_ids = self.model.generate(
            inputs,
            max_length=max_length,
            min_length=min_length,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
        
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        return SummaryResult(
            summary=summary,
            method="BART",
            original_length=len(text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(text),
            sentences_used=[summary]
        )


# ============================================================================
# ABSTRACTIVE SUMMARIZATION - PEGASUS MODEL
# ============================================================================

class PegasusSummarizer:
    """Abstractive summarization using PEGASUS model"""
    
    def __init__(self, model_name: str = "google/pegasus-xsum"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library is required for PEGASUS summarization")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)
        self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.device)
    
    def summarize(
        self, 
        text: str, 
        max_length: int = 150, 
        min_length: int = 40
    ) -> SummaryResult:
        """Generate abstractive summary using PEGASUS"""
        inputs = self.tokenizer.encode(
            text, 
            return_tensors="pt", 
            max_length=512, 
            truncation=True
        ).to(self.device)
        
        summary_ids = self.model.generate(
            inputs,
            max_length=max_length,
            min_length=min_length,
            length_penalty=0.8,
            num_beams=8,
            early_stopping=True
        )
        
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        return SummaryResult(
            summary=summary,
            method="PEGASUS",
            original_length=len(text),
            summary_length=len(summary),
            compression_ratio=len(summary) / len(text),
            sentences_used=[summary]
        )


# ============================================================================
# UNIFIED TEXT ABSTRACTOR
# ============================================================================

class TextAbstractor:
    """
    Unified interface for text summarization with multiple methods
    """
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        
        # Initialize extractive summarizers
        self.tfidf = TFIDFSummarizer(self.preprocessor)
        self.textrank = TextRankSummarizer(self.preprocessor)
        self.lexrank = LexRankSummarizer(self.preprocessor)
        
        if SKLEARN_AVAILABLE:
            self.lsa = LSASummarizer(self.preprocessor)
        else:
            self.lsa = None
        
        # Abstractive summarizers (lazy loading)
        self._t5 = None
        self._bart = None
        self._pegasus = None
    
    @property
    def t5(self):
        if self._t5 is None and TRANSFORMERS_AVAILABLE:
            self._t5 = T5Summarizer()
        return self._t5
    
    @property
    def bart(self):
        if self._bart is None and TRANSFORMERS_AVAILABLE:
            self._bart = BARTSummarizer()
        return self._bart
    
    @property
    def pegasus(self):
        if self._pegasus is None and TRANSFORMERS_AVAILABLE:
            self._pegasus = PegasusSummarizer()
        return self._pegasus
    
    def summarize(
        self,
        text: str,
        method: str = "tfidf",
        num_sentences: int = 3,
        max_length: int = 150,
        min_length: int = 40
    ) -> SummaryResult:
        """
        Generate summary using specified method
        
        Args:
            text: Input text to summarize
            method: Summarization method ('tfidf', 'textrank', 'lexrank', 'lsa', 
                   't5', 'bart', 'pegasus')
            num_sentences: Number of sentences for extractive methods
            max_length: Maximum length for abstractive methods
            min_length: Minimum length for abstractive methods
        
        Returns:
            SummaryResult object containing the summary and metadata
        """
        method = method.lower()
        
        # Extractive methods
        if method == "tfidf":
            return self.tfidf.summarize(text, num_sentences)
        elif method == "textrank":
            return self.textrank.summarize(text, num_sentences)
        elif method == "lexrank":
            return self.lexrank.summarize(text, num_sentences)
        elif method == "lsa":
            if self.lsa is None:
                raise ImportError("scikit-learn required for LSA")
            return self.lsa.summarize(text, num_sentences)
        
        # Abstractive methods
        elif method == "t5":
            if self.t5 is None:
                raise ImportError("transformers required for T5")
            return self.t5.summarize(text, max_length, min_length)
        elif method == "bart":
            if self.bart is None:
                raise ImportError("transformers required for BART")
            return self.bart.summarize(text, max_length, min_length)
        elif method == "pegasus":
            if self.pegasus is None:
                raise ImportError("transformers required for PEGASUS")
            return self.pegasus.summarize(text, max_length, min_length)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def compare_methods(
        self,
        text: str,
        methods: List[str] = None,
        num_sentences: int = 3
    ) -> Dict[str, SummaryResult]:
        """
        Compare multiple summarization methods
        
        Args:
            text: Input text to summarize
            methods: List of methods to compare (default: all available extractive)
            num_sentences: Number of sentences for extractive methods
        
        Returns:
            Dictionary mapping method names to SummaryResult objects
        """
        if methods is None:
            methods = ["tfidf", "textrank", "lexrank"]
            if SKLEARN_AVAILABLE:
                methods.append("lsa")
        
        results = {}
        for method in methods:
            try:
                results[method] = self.summarize(text, method, num_sentences)
            except Exception as e:
                print(f"Error with {method}: {e}")
        
        return results
    
    def multi_document_summarize(
        self,
        documents: List[str],
        method: str = "tfidf",
        num_sentences: int = 5
    ) -> SummaryResult:
        """
        Summarize multiple documents
        
        Args:
            documents: List of document texts
            method: Summarization method to use
            num_sentences: Number of sentences in final summary
        
        Returns:
            SummaryResult object
        """
        # Combine all documents
        combined_text = " ".join(documents)
        return self.summarize(combined_text, method, num_sentences)


# ============================================================================
# EXAMPLE USAGE AND MAIN FUNCTION
# ============================================================================

def main():
    """Example usage of the TextAbstractor"""
    
    # Sample text for demonstration
    sample_text = """
    Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to 
    the natural intelligence displayed by humans and animals. Leading AI textbooks define 
    the field as the study of "intelligent agents": any device that perceives its environment 
    and takes actions that maximize its chance of successfully achieving its goals. 
    Colloquially, the term "artificial intelligence" is often used to describe machines 
    (or computers) that mimic "cognitive" functions that humans associate with the human 
    mind, such as "learning" and "problem solving".
    
    As machines become increasingly capable, tasks considered to require "intelligence" 
    are often removed from the definition of AI, a phenomenon known as the AI effect. 
    A quip in Tesler's Theorem says "AI is whatever hasn't been done yet." For instance, 
    optical character recognition is frequently excluded from things considered to be AI, 
    having become a routine technology.
    
    Modern machine learning algorithms are capable of learning from experience and can 
    perform sophisticated tasks such as image recognition, natural language processing, 
    and game playing. Deep learning, a subset of machine learning based on artificial 
    neural networks, has been particularly successful in recent years, achieving 
    breakthrough results in various domains.
    """
    
    print("=" * 80)
    print("TEXT ABSTRACTOR - Comprehensive Summarization Tool")
    print("=" * 80)
    print()
    
    # Initialize the abstractor
    abstractor = TextAbstractor()
    
    # Test extractive methods
    print("EXTRACTIVE SUMMARIZATION METHODS:")
    print("-" * 80)
    
    extractive_methods = ["tfidf", "textrank", "lexrank"]
    if SKLEARN_AVAILABLE:
        extractive_methods.append("lsa")
    
    for method in extractive_methods:
        print(f"\n{method.upper()} Summary:")
        try:
            result = abstractor.summarize(sample_text, method=method, num_sentences=2)
            print(f"Summary: {result.summary}")
            print(f"Compression Ratio: {result.compression_ratio:.2%}")
        except Exception as e:
            print(f"Error: {e}")
    
    # Test abstractive methods if available
    if TRANSFORMERS_AVAILABLE:
        print("\n" + "=" * 80)
        print("ABSTRACTIVE SUMMARIZATION METHODS:")
        print("-" * 80)
        
        print("\nNote: Abstractive methods require model downloads and may take time on first run.")
        print("Using T5 as an example (comment out if models are too large for your system):")
        
        # Uncomment to test T5
        # try:
        #     result = abstractor.summarize(sample_text, method="t5", max_length=100)
        #     print(f"\nT5 Summary: {result.summary}")
        #     print(f"Compression Ratio: {result.compression_ratio:.2%}")
        # except Exception as e:
        #     print(f"Error: {e}")
    
    # Compare methods
    print("\n" + "=" * 80)
    print("COMPARING MULTIPLE METHODS:")
    print("-" * 80)
    
    comparison = abstractor.compare_methods(sample_text, num_sentences=2)
    
    for method, result in comparison.items():
        print(f"\n{method.upper()}:")
        print(f"  Summary: {result.summary[:100]}...")
        print(f"  Compression: {result.compression_ratio:.2%}")
        print(f"  Length: {result.summary_length} chars")
    
    print("\n" + "=" * 80)
    print("Example completed successfully!")
    print("=" * 80)


if __name__ == "__main__":
    main()


# ============================================================================
# INSTALLATION INSTRUCTIONS
# ============================================================================
"""
INSTALLATION:

1. Basic Installation (Extractive methods only):
   pip install nltk numpy

2. Full Installation (All extractive methods):
   pip install nltk numpy scikit-learn networkx

3. Complete Installation (Including abstractive methods):
   pip install nltk numpy scikit-learn networkx transformers torch

4. For GPU support (faster abstractive summarization):
   pip install nltk numpy scikit-learn networkx transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

USAGE:

from text_abstractor import TextAbstractor

# Initialize
abstractor = TextAbstractor()

# Extractive summarization
result = abstractor.summarize(your_text, method="tfidf", num_sentences=3)
print(result.summary)

# Abstractive summarization (requires transformers)
result = abstractor.summarize(your_text, method="t5", max_length=150)
print(result.summary)

# Compare methods
results = abstractor.compare_methods(your_text)
for method, result in results.items():
    print(f"{method}: {result.summary}")

METHODS AVAILABLE:
- Extractive: tfidf, textrank, lexrank, lsa
- Abstractive: t5, bart, pegasus

Each method has different strengths:
- TF-IDF: Fast, good for factual content
- TextRank: Good for diverse content selection
- LexRank: Excellent for multi-document summarization
- LSA: Good for topic-based summarization
- T5: Versatile, good general-purpose abstractive
- BART: Excellent for news articles
- PEGASUS: State-of-the-art for extreme summarization
"""
